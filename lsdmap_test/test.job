#!/bin/bash
#----------------------------------------------------
# Example SLURM job script to run large MPI
# applications in the large queue on TACC's Stampede
# system.
#----------------------------------------------------
#SBATCH -J lsdmap        # Job name
#SBATCH -o lsdmap.o%j    # Name of stdout output file(%j expands to jobId)
#SBATCH -e lsdmap.e%j    # Name of stderr output file(%j expands to jobId)
#SBATCH -p development            # submit to the 'large' queue for jobs > 256 nodes
#SBATCH -N 4              # Total number of nodes requested (16 cores/node)
#SBATCH -n 64             # Total number of mpi tasks requested
#SBATCH -t 00:30:00         # Run time (hh:mm:ss) - 1.5 hours
#The next line is required if the user has more than one project
#SBATCH -A TG-MCB090174 # Allocation name to charge job against


cd $HOME/lsdmap_test
module load -intel +intel/13.0.2.146
module load gromacs
echo 2 | trjconv -f tmp.gro -s tmp.gro -o tmpha.gro &>/dev/null
module load -intel +intel/14.0.1.106
module load python

for i in 1 2; do
	/usr/local/bin/ibrun -n 64 -o 0 /opt/apps/intel14/mvapich2_2_0/python/2.7.6/lib/python2.7/site-packages/mpi4py/bin/python-mpi lsdm.py -f config.ini -c tmpha.gro -n neighbors.nn -w weight.w
	wait
	echo "done"
done

